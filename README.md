## ðŸ“– Project Overview

In this project, users upload a video file, and the app first extracts and transcribes the audio using OpenAIâ€™s Whisper model. The raw transcription is then corrected using a T5-based grammar correction model to improve grammar, spelling, and fluency, while also highlighting the changes compared to the original text. A 5-second audio clip from the uploaded video is trimmed to serve as a voice reference. Using the XTTS-v2 model from Coqui, the app performs voice cloning to synthesize the corrected text into natural-sounding speech that matches the original speakerâ€™s voice. Finally, the generated voice audio is made available for playback directly within the app, completing an end-to-end process of transcription, grammar enhancement, and speaker-adapted text-to-speech synthesis.

---

## ðŸ“‚ Project Structure

. 
â”œâ”€â”€ Assets_Text.py # Utility functions: ASR, formatting, correction, highlighting 
â”œâ”€â”€ requirements.txt # Python dependencies 
â”œâ”€â”€ Streamlit_App.py # Main Streamlit application 
â”œâ”€â”€ saved_output/ # Auto-generated transcripts, audio & TTS outputs â”‚ 
â”‚  â”œâ”€â”€ extracted_audio_segments/ # Audio clips for each subtitle â”‚ 
â”‚  â”œâ”€â”€ transcribed_subtitles.srt # Raw transcription (SRT format) â”‚ 
â”‚  â”œâ”€â”€ incorrect_paragraph.txt # Original, uncorrected text â”‚ 
â”‚  â”œâ”€â”€ corrected_paragraph.txt # Corrected text after grammar fixing â”‚ 
â”‚  â”œâ”€â”€ full_audio.wav # Full extracted audio from uploaded video â”‚ 
â”‚  â”œâ”€â”€ trimmed_ref.wav # 5-second trimmed reference audio for voice cloning 
â”‚ â””â”€â”€ output.wav # Final synthesized voice output 
â””â”€â”€ .env # Environment variables (e.g., Hugging Face token, not committed)
