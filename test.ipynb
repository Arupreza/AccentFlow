{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file using FFmpeg.\n",
    "    \"\"\"\n",
    "    ffmpeg.input(video_path).output(output_audio_path, format=\"wav\").run(overwrite_output=True)\n",
    "    return output_audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio_path, output_folder=\"audio_chunks\", chunk_length_ms=30000):\n",
    "    \"\"\"\n",
    "    Splits the extracted audio into 30-second chunks and saves them.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    chunks = [audio[i : i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "\n",
    "    chunk_paths = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join(output_folder, f\"chunk_{idx}.wav\")\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "        chunk_paths.append(chunk_filename)\n",
    "    \n",
    "    return chunk_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_chunks, output_folder=\"audio_chunks\"):\n",
    "    \"\"\"\n",
    "    Transcribes speech from multiple short audio chunks and saves text files.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load Whisper model\n",
    "    asr_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "    transcriptions = []\n",
    "    \n",
    "    for idx, chunk in enumerate(audio_chunks):\n",
    "        result = asr_pipeline(chunk)\n",
    "        text = result[\"text\"]\n",
    "\n",
    "        # Save each transcription next to its corresponding audio file\n",
    "        text_filename = os.path.join(output_folder, f\"chunk_{idx}.txt\")\n",
    "        with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "        transcriptions.append((chunk, text, text_filename))\n",
    "    \n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_folder=\"output\"):\n",
    "    \"\"\"\n",
    "    Full pipeline: Extracts audio, splits it into chunks, and transcribes speech.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "\n",
    "    # Step 1: Extract audio\n",
    "    audio_path = extract_audio(video_path, os.path.join(output_folder, \"full_audio.wav\"))\n",
    "    print(f\"Audio extracted: {audio_path}\")\n",
    "\n",
    "    # Step 2: Split into 30-second chunks\n",
    "    audio_chunks = split_audio(audio_path, output_folder)\n",
    "    print(f\"Audio split into {len(audio_chunks)} chunks.\")\n",
    "\n",
    "    # Step 3: Transcribe each chunk and save next to audio\n",
    "    transcript_info = transcribe_audio(audio_chunks, output_folder)\n",
    "\n",
    "    print(\"\\nâœ… Process Completed!\")\n",
    "    for chunk, text, text_file in transcript_info:\n",
    "        print(f\"ðŸ”¹ Audio: {chunk} -> ðŸ”¹ Text: {text_file}\")\n",
    "\n",
    "    return audio_chunks, transcript_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Sample_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Sample_1.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    creation_time   : 2025-01-16T05:28:25.000000Z\n",
      "  Duration: 00:02:52.53, start: 0.000000, bitrate: 2199 kb/s\n",
      "  Stream #0:0(und): Video: hevc (Main) (hvc1 / 0x31637668), yuv420p(tv, bt709), 1280x720, 2064 kb/s, 29.99 fps, 30 tbr, 600 tbn, 600 tbc (default)\n",
      "    Metadata:\n",
      "      rotate          : 180\n",
      "      creation_time   : 2025-01-16T05:28:25.000000Z\n",
      "      handler_name    : Core Media Video\n",
      "      vendor_id       : [0][0][0][0]\n",
      "    Side data:\n",
      "      displaymatrix: rotation of -180.00 degrees\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 129 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-01-16T05:28:25.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'transcriptions/full_audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 1\n",
      "    compatible_brands: isommp41mp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-01-16T05:28:25.000000Z\n",
      "      handler_name    : Core Media Audio\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=   29724kB time=00:02:52.52 bitrate=1411.4kbits/s speed= 865x    \n",
      "video:0kB audio:29724kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000256%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted: transcriptions/full_audio.wav\n",
      "Audio split into 6 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "/home/arupreza/anaconda3/envs/acc_env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Process Completed!\n",
      "ðŸ”¹ Audio: transcriptions/chunk_0.wav -> ðŸ”¹ Text: transcriptions/chunk_0.txt\n",
      "ðŸ”¹ Audio: transcriptions/chunk_1.wav -> ðŸ”¹ Text: transcriptions/chunk_1.txt\n",
      "ðŸ”¹ Audio: transcriptions/chunk_2.wav -> ðŸ”¹ Text: transcriptions/chunk_2.txt\n",
      "ðŸ”¹ Audio: transcriptions/chunk_3.wav -> ðŸ”¹ Text: transcriptions/chunk_3.txt\n",
      "ðŸ”¹ Audio: transcriptions/chunk_4.wav -> ðŸ”¹ Text: transcriptions/chunk_4.txt\n",
      "ðŸ”¹ Audio: transcriptions/chunk_5.wav -> ðŸ”¹ Text: transcriptions/chunk_5.txt\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"Sample_1.mp4\"  # Replace with your video file path\n",
    "    process_video(video_file, output_folder=\"transcriptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths\n",
    "data_dir = \"/media/arupreza/Assets/LLM Projects/AccentFlow/transcriptions\"\n",
    "output_dir = os.path.join(data_dir, \"corrected_texts\")  # Save corrected texts here\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Grammarly CoEdit-Large Model (T5-based)\n",
    "model_name = \"grammarly/coedit-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Wrap Hugging Face model into LangChain\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name, \n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LangChain Prompt\n",
    "grammar_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Fix grammar: {text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Chain for Grammar Correction\n",
    "grammar_chain = LLMChain(llm=llm, prompt=grammar_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar_langchain(text):\n",
    "    \"\"\"\n",
    "    Uses LangChain with Grammarly CoEdit-Large to correct grammar.\n",
    "    \"\"\"\n",
    "    response = grammar_chain.run({\"text\": text})\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    Processes all .txt files in the input directory using LangChain, corrects grammar, and saves output.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename.replace(\".txt\", \"_corrected.txt\"))\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            corrected_text = correct_grammar_langchain(text)\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(corrected_text)\n",
    "\n",
    "            print(f\"âœ… Processed: {filename} â†’ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LangChain-based processing function\n",
    "process_text_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
